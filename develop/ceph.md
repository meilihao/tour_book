# ceph
Ceph是一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式的存储系统.

存储根据其类型，可分为块存储，对象存储和文件存储. 在主流的分布式存储技术中，HDFS/GPFS/GFS属于文件存储，Swift属于对象存储，而Ceph可支持块存储、对象存储和文件存储，故称为统一存储.

![Ceph的主要架构](/misc/img/ceph/Image00152.jpg)

架构:
1. Ceph的最底层和核心是RADOS（分布式对象存储系统），它具有可靠、智能(自动化)、分布式等特性，实现高可靠、高可拓展、高性能、高自动化等功能，并最终提供了一个可无限扩展的存储集群. 

	RADOS由大量的存储设备节点组成，每个节点拥有自己的硬件资源（CPU、内存、硬盘、网络），并运行着操作系统和文件系统.

	ceph采用具有计算能力的设备（比如普通的服务器）作为存储系统的存储节点, 是为了充分发挥存储设备自身的计算能力.

	RADOS集群主要由两种节点组成：为数众多的OSD，负责完成数据存储和维护；若干个Monitor，负责完成系统状态检测和维护. OSD和Monitor之间互相传递节点的状态信息，共同得出系统的总体运行状态，并保存在一个全局的数据结构中，即所谓的集群运行图（Cluster Map）里. 集群运行图与RADOS提供的特定算法相配合，便实现了Ceph的许多优秀特性.

	据定义，OSD可以被抽象为系统和守护进程（OSD Daemon）两个部分:
	1. OSD的系统部分本质上就是一台安装了操作系统和文件系统的计算机
	1. 每个OSD占用一定的计算能力、一定数量的内存, 一块硬盘（在通常情况下一个OSD对应一块硬盘）, 及足够的网络带宽.

		每个OSD拥有一个自己的OSD Daemon. 这个Daemon负责完成OSD的所有逻辑功能，包括与Monitor和其他OSD（事实上是其他OSD的Daemon）通信，以维护及更新系统状态，与其他OSD共同完成数据的存储和维护操作，与客户端通信完成各种数据对象操作，等等.

	RADOS集群从Ceph客户端接收数据（无论是来自Ceph块设备、Ceph对象存储、Ceph文件系统，还是基于librados的自定义实现），然后存储为对象. 每个对象是文件系统中的一个文件，它们存储在OSD的存储设备上，由OSD Daemon处理存储设备上的读/写操作.

	OSD在扁平的命名空间内把所有数据存储为对象（也就是没有目录层次）. 对象包含一个标识符、二进制数和由名/值对组成的元数据，元数据语义完全取决于Ceph客户端.

1. RADOS之上是LIBRADOS，LIBRADOS是一个库，它允许应用程序通过访问该库来与RADOS系统进行交互，支持多种编程语言，比如C、C++、Python等

	librados库实际上是对RADOS进行抽象和封装，并向上层提供API的，以便可以基于RADOS（而不是整个Ceph）进行应用开发. 特别要注意的是，RADOS是一个对象存储系统，因此，librados库实现的API也**只是针对对象存储功能**的.
1. 基于LIBRADOS层开发的有三种接口，分别是RADOSGW、RBD与Ceph FS
	
	1. RADOSGW是一套基于当前流行的RESTFUL协议的网关，支持对象存储，兼容S3和Swift
	1. RBD提供分布式的块存储设备接口，支持块存储. 常用于在虚拟化的场景下为虚拟机创建存储卷。Red Hat已经将RBD驱动集成在KVM/QEMU中，以提高虚拟机的访问性能.
	1. Ceph FS提供兼容POSIX的文件系统，支持文件存储

	因此严格意义上讲，Ceph只提供对象存储接口，所谓的块存储接口和文件系统存储接口都算是对象存储接口应用程序.

## 组件
- Client : 负责存储协议的接入，节点负载均衡
- Monitors: Ceph 监视器(ceph-mon) 

    负责系统状态检测和维护. 它维护着集群状态的各种运行图，包括如OSD Map、Monitor Map、PG Map和CRUSH Map，这些运行图都是很要紧的集群状态，对于各种 Ceph 守护进程的相互协作必不可少. 监视器还负责管理守护进程和客户端之间的认证. 考虑到冗余性和高可用性，一般都要求至少有三个监视器.
- Managers: Ceph 管理器守护进程（ ceph-mgr ）

    负责持续跟踪运行时指标和 Ceph 当前的状态，包括存储利用率、当前的性能指标、和系统负载. Ceph 管理器守护进程还托管着基于 python 的插件，用于管理和展示 Ceph 集群信息，包括一个基于网页的 Ceph 管理器仪表盘 和 REST API. 为保障高可用性，一般要求至少有两个管理器.
- Ceph OSDs: Ceph OSD （对象存储守护进程， ceph-osd ）

    负责数据存储和维护(数据复制、恢复、重均衡, 以及向 Ceph 监视器和管理器提供些监控信息，如检查其它 Ceph OSD 守护进程的心跳等). 一般情况下一块硬盘对应一个OSD. 因此为保障冗余性和高可用性，一般需要至少 3 个 Ceph OSD.
- MDSs: Ceph 元数据服务器（ MDS ， ceph-mds ）, Ceph 文件系统客户端依赖它.

    为 Ceph 文件系统存储元数据（也就是说， Ceph 块设备和 Ceph 对象存储不使用 MDS ）比如目录结构. 元数据服务器有益于 POSIX 文件系统用户执行基本命令（像 ls 、 find 等等），避免了给 Ceph 存储集群增加过重的负担.

Ceph 把数据保存为逻辑存储池内的对象. 根据 CRUSH 算法， Ceph 可计算出哪个归置组应该持有指定对象，然后进一步计算出哪个 OSD 守护进程持有归置组，正因为有了 CRUSH 算法， Ceph 存储集群才具备动态伸缩、重均衡和动态恢复功能.

> 新版ceph的cephfs基于bluestore, 不再依赖其他文件系统.

## OSD
OSD的状态直接影响数据的重新分配，所以监测OSD的状态是Monitor的主要工作之一.

OSD状态用两个维度表示：up或down（OSD Daemon与Monitor连接是否正常）；in或out（OSD是否含有PG）. 因此，对于任意一个OSD，共有4种可能的状态:
- up & out：OSD Daemon与Monitor通信正常，但是没有PG分配到该OSD上。这种状态一般是OSD Daemon刚刚启动时
- up & in：OSD Daemon工作的正常状态，有PG分配到OSD上
- down & in：OSD Daemon不能与Monitor或其他OSD进行正常通信，这可能是因为网络中断或Daemon进程意外退出
- down & out：OSD无法恢复，Monitor决定将OSD上的PG进行重新分配。之所以会出现该状态，是考虑OSD可能会在短时间内恢复，尽量减少数据的再分配

OSD状态是通过心跳（Heartbeat）检测的:
- Peer OSD之间的心跳包

	Peer OSD是指该OSD上所有PG的副本所在的OSD. 同时由于Ceph提供公众网络（Public Network）（OSD与客户端通信）和集群网络（Cluster Network）（OSD之间的通信），所以Peer OSD之间的心跳包也分为前端（公众网络）和后端（集群网络），这样可最大限度地监测OSD及公众网络和集群网络的状态，及时上报Monitor. 同时考虑到网络的抖动问题，可以设置Monitor在决定OSD下线之前需要收到多少次的报告.
- OSD与Monitor之间的心跳包

	这个心跳包可以看作是Peer OSD之间心跳包的补充. 如果OSD不能与其他OSD交换心跳包，那么就必须与Monitor按照一定频率进行通信，比如OSD状态是up & out时就需要这种心跳包

### Ceph寻址流程
![](/misc/img/io/Image00156.jpg)

Ceph寻址流程涉及的概念:
- File：此处的File就是用户需要存储或访问的文件. 对于一个基于Ceph开发的对象存储应用而言，这个File也就对应于应用中的“对象”，也就是用户直接操作的“对象”.
- Object：此处的Object是RADOS所看到的“对象”. Object与File的区别是，Object的最大尺寸由RADOS限定（通常为2MB或4MB），以便实现底层存储的组织管理. 因此，当上层应用向RADOS存入尺寸很大的File时，需要将File切分成统一大小的一系列Object（最后一个的大小可以不同）进行存储.
- PG（Placement Group）：顾名思义，PG的用途是对Object的存储进行组织和位置映射的. 具体而言，一个PG负责组织若干个Object（可以为数千个甚至更多），但一个Object只能被映射到一个PG中，即PG和Object之间是“一对多”的映射关系. 同时，一个PG会被映射到n 个OSD上，而每个OSD上都会承载大量的PG，即PG和OSD之间是“多对多”的映射关系. 在实践当中，n 至少为2，如果用于生产环境，则至少为3. 一个OSD上的PG可达到数百个. 事实上，PG数量的设置关系到数据分布的均匀性问题.
- OSD：OSD的数量事实上也关系到系统的数据分布均匀性，因此不应该太少. 在实践当中，至少也应该是数百个的量级才有助于Ceph系统发挥其应有的优势.

具体映射:
1. File→Object映射
	
	目的: 将用户要操作的File映射为RADOS能够处理的Object，其十分简单，本质上就是按照Object的最大尺寸对File进行切分，相当于磁盘阵列中的条带化过程.

	这种切分的好处有两个：一是让大小不限的File变成具有一致的最大尺寸、可以被RADOS高效管理的Object；二是让对单一File实施的串行处理变为对多个Object实施的并行化处理.

	每一个切分后产生的Object将获得唯一的oid，即Object ID.

	ino是待操作File的元数据，可以简单理解为该File的唯一ID, 因此ino的唯一性必须得到保证. ono则是由该File切分产生的某个Object的序号. 而oid就是将这个序号简单连缀在该File ID之后得到的.
1. Object → PG映射

	公式: `hash(oid) & mask -> pgid`

	首先，使用Ceph系统指定的一个静态哈希算法计算oid的哈希值，将oid映射为一个近似均匀分布的伪随机值. 然后，将这个伪随机值和mask按位相与，得到最终的PG序号（pgid）.

	根据RADOS的设计，给定PG的总数为m （m 应该为2的整数幂），则mask的值为m- 1. 因此，哈希值计算和按位与操作的整体结果事实上是从所有m 个PG中近似均匀地随机选择1个. 基于这一机制，当有大量Object和大量PG时，RADOS能够保证Object和PG之间的近似均匀映射. 又因为Object是由File切分而来的，大部分Object的尺寸相同，因此，这一映射最终保证了各个PG中存储的Object的总数据量近似均匀.

	这里反复强调了“大量”，意思是只有当Object和PG的数量较多时，这种伪随机关系的近似均匀性才能成立，Ceph的数据存储均匀性才有保证. 为保证“大量”的成立，一方面，Object的最大尺寸应该被合理配置，以使得同样数量的File能够被切分成更多的Object；另一方面，Ceph也推荐PG总数应该为OSD总数的数百倍，以保证有足够数量的PG可供映射.
1. PG → OSD映射

	将作为Object的逻辑组织单元的PG映射到数据的实际存储单元OSD上. RADOS采用一个名为CRUSH的算法，将pgid代入其中，然后得到一组共n 个OSD. 这n 个OSD共同负责存储和维护一个PG中的所有Object. n 的数值可以根据实际应用中对于可靠性的需求而配置，在生产环境下通常为3. 具体到每个OSD，则由其上运行的OSD Daemon负责执行映射到本地的Object在本地文件系统中的存储、访问、元数据维护等操作.

	和“Object → PG”映射中采用的哈希算法不同，CRUSH算法的结果不是绝对不变的，而会受到其他因素的影响。其影响因素主要有两个:
	1. 当前系统状态(集群运行图). 当系统中的OSD状态、数量发生变化时，集群运行图也可能发生变化，而这种变化将会影响到PG与OSD之间的映射关系
	1. 存储策略配置. 这里的策略主要与安全相关. 利用策略配置，系统管理员可以指定承载同一个PG的3个OSD分别位于数据中心的不同服务器或机架上，从而进一步改善存储的可靠性.

	Ceph正是利用了CRUSH算法的动态特性，可以将一个PG根据需要动态迁移到不同的OSD组合上，从而自动化地实现高可靠性、数据分布再平衡等特性.

从整个过程可以看到，这里没有任何的全局性查表操作需求. 至于唯一的全局性数据结构：集群运行图, 它的维护和操作都是轻量级的，不会对系统的可扩展性、性能等因素造成影响.

引入PG的好处至少有两方面：
1. 一方面实现了Object和OSD之间的动态映射，从而为Ceph的可靠性、自动化等特性的实现留下了空间

	如果Object直接映射到一组OSD上, 这种算法是某种固定映射的哈希算法, 那么osd损坏或新增osd, object没法迁移或再平衡到新osd.
1. 另一方面也有效简化了数据的存储组织，大大降低了系统的维护与管理成本

	如果Object直接映射到一组OSD上, 这种算法是某种动态算法, 比如仍然采用CRUSH算法. 在Ceph的现有机制中，一个OSD平时需要和与其共同承载同一个PG的其他OSD交换信息，以确定各自是否工作正常，是否需要进行维护操作. 如果没有pg, 则一个OSD需要和与其共同承载同一个Object的其他OSD交换信息, 由于每个OSD上承载的Object可能高达数百万个, 因此，同样长度的一段时间内，一个OSD大约需要进行的OSD间信息交换将暴涨至数百万次乃至数千万次. 而这种状态维护成本显然过高.

这种分层或分级的设计思路在很多复杂系统的寻址问题上都有应用，比如操作系统里的内存管理多级页表的使用，英特尔MPX（Memory Protection Extensions）技术里引入的Bound Directory等.

### 存储池
ceph存储池是一个逻辑概念，是对存储对象的逻辑分区. Ceph安装后，会有一个默认的存储池，用户也可以自己创建新的存储池. 一个存储池包含若干个PG及其所存储的若干个对象.

Ceph客户端从监视器获取一张集群运行图，并把对象写入存储池. 存储池的大小或副本数、CRUSH存储规则和归置组数量决定Ceph如何放置数据. ceph中通过`ceph osd pool crate`创建存储池.

创建存储池命令支持的参数如下:
- 设置数据存储的方法属于多副本模式还是纠删码模式. 如果是多副本模式，则可以设置副本的数量；如果是纠删码模式，则可以设置数据块和非数据块的数量（纠删码存储池把各对象存储为K +M 个数据块，其中有K 个数据块和M个编码块）. 默认为多副本模式（即存储每个对象的若干个副本），如果副本数为3，则每个PG映射到3个OSD节点上
- 设置PG的数目. 合理设置PG的数目，可以使资源得到较优的均衡
- 设置PGP的数目. 在通常情况下，与PG数目一致. 当需要增加PG数目时，用户数据不会发生迁移，只有进一步增加PGP数目时，用户数据才会开始迁移
- 针对不同的存储池设置不同的CRUSH存储规则. 比如可以创建规则，指定在选择OSD时，选择拥有固态硬盘的OSD节点

另外，通过存储池，还可以进行如下操作:
1. 提供针对存储池的功能，如存储池快照等
1. 设置对象的所有者或访问权限

PGP是存储池PG的OSD分布组合个数. PG数目的增加会引起PG的分裂，新的PG仍然在原来的OSD上，而PGP数目的增加则会引起部分PG的分布发生变化, 但是不会引起PG内对象的变动. 可参考[ceph分布式存储-PG和PGP的区别](https://cloud.tencent.com/developer/article/1664635)

## FAQ
### DRBD vs Ceph
CEPH 是一种开源软件，旨在在统一系统中提供高度可扩展的对象，块和基于文件的存储.

DRBD 是一种基于软件和网络(tcp/ip和RDMA)的块复制存储解决方案, 基于块设备, 相对而言, 其在ceph的块设备之上.